{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Deep Convolutional Networks for Demosaicing\n",
    "\n",
    "> Original paper by Nai-Sheng Syu, Yu-SHeng Chen, Yung-Yu Chuang [[1]](https://arxiv.org/pdf/1802.03769.pdf)\n",
    "\n",
    "### About the notebook\n",
    "\n",
    "This notebook and repository aims to reproduce the state-of-the-art results provided in the [1]. \n",
    "\n",
    "**Author:** Eemeli Saari\n",
    "\n",
    "**Email:** saari.eemeli@gmail.com\n",
    "\n",
    "**Modified:** 21.4.2019\n",
    "\n",
    "> **NOTE:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set a standard seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f37226b7690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We load the data from the authors homepage and define a Pytorch dataset that reads patches from the images.\n",
    "\n",
    "The image patches are densely extracted $33 \\times 33$ patches that are padded with zeros to make up for the missing channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_data, ImagePatchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 14.5 ms, total: 37.5 ms\n",
      "Wall time: 41.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8526"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dataset = ImagePatchDataset(root=download_data(), sample_size=2)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMCNN Model\n",
    "\n",
    "We'll first define the DMCNN model described by the paper [1].\n",
    "\n",
    "Model contains\n",
    "\n",
    "- Feature extraction layer\n",
    "  - 128 Filters with $9\\times9$ kernels.\n",
    "- Non-linear mapping layer\n",
    "  - 64 Filters with $1\\times1$ kernels.\n",
    "- Reconstruction layer\n",
    "  - 3 Kernels constructing the resulted colored image\n",
    "  - $5\\times5$ kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMCNN(torch.nn.Module):\n",
    "    \"\"\"DMCNN\n",
    "    \n",
    "    Model adopted from [link](https://arxiv.org/pdf/1802.03769.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DMCNN, self).__init__()\n",
    "        \n",
    "        self.feature_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 128, kernel_size=9),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mapping_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 64, kernel_size=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.reconstruction_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=5),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.feature_layer(x)\n",
    "        out = self.mapping_layer(out)\n",
    "        out = self.reconstruction_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 ms, sys: 2.7 ms, total: 4.36 ms\n",
      "Wall time: 3.67 ms\n"
     ]
    }
   ],
   "source": [
    "%time model = DMCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 25, 25]          31,232\n",
      "              ReLU-2          [-1, 128, 25, 25]               0\n",
      "            Conv2d-3           [-1, 64, 25, 25]           8,256\n",
      "              ReLU-4           [-1, 64, 25, 25]               0\n",
      "            Conv2d-5            [-1, 3, 21, 21]           4,803\n",
      "              ReLU-6            [-1, 3, 21, 21]               0\n",
      "================================================================\n",
      "Total params: 44,291\n",
      "Trainable params: 44,291\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.85\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 2.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 33, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it's a fairly shallow convolution neural network that should be fairly fast to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The Pytorch framework doesn't offer the similar sklearn type API that's supported in the Keras so we'll need to manually create the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll define the the loss function as the standard $L_2$ norm.\n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{n} \\sum_{n}^{n=1}||F(Y_i ; \\theta) - X_i||^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original paper the DMCNN is trained using the stochastic gradient descent with learning rate 1 for the first two layers and 0.1 for the last layer.\n",
    "\n",
    "This comes easy to define in Pytorch as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    params = [\n",
    "        {\"params\": model.feature_layer.parameters(), \"lr\": 1},\n",
    "        {\"params\": model.mapping_layer.parameters(), \"lr\": 1},\n",
    "        {\"params\": model.reconstruction_layer.parameters(), \"lr\": 0.1}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the training loop to include the clipping to keep the gradients within -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8ee9ba1ea814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcfa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = []\n",
    "    for idx, (cfa, target) in enumerate(data_loader):\n",
    "        outputs = model(cfa)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss_list.append(loss.item())\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            #print(outputs)\n",
    "            print(f'Epoch [{epoch}/{n_epochs}], Step [{idx}/{total_step}], Loss: {loss.item()}')\n",
    "    epoch_stats = np.array(epoch_loss)\n",
    "    print(f'\\nFinished Epoch {epoch}, Loss --- mean: {epoch_stats.mean()}, std {epoch_stats.std()}\\n')\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,8))\n",
    "    ax1.imshow(np.array(outputs[-1].tolist()).reshape((21, 21, 3)))\n",
    "    ax2.imshow(np.array(cfa[-1].tolist()).reshape((33, 33, 3)))\n",
    "    ax3.imshow(np.array(target[-1].tolist()).reshape((21, 21, 3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_to_tensor(patch):\n",
    "    shape = list(patch.shape)\n",
    "    tensor = torch.from_numpy(patch.reshape(tuple(reversed(shape))))\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 21, 21])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_to_tensor(np.zeros((21, 21, 3))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demosaic(model, cfa):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMCNN-VD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMCNN_VD(torch.nn.Module):\n",
    "    \"\"\"DMCNN-VD\n",
    "\n",
    "    Model adopted from [link](https://arxiv.org/pdf/1802.03769.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers=20):\n",
    "        super(DMCNN_VD, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.layer0 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.SELU()\n",
    "        )\n",
    "        for i in range(1, self.n_layers):\n",
    "            setattr(self, f'layer{i}', self.conv_layer)\n",
    "\n",
    "        # https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/deep_residual_network/main.py\n",
    "        self.residual = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(3),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Conv2d(3, 3, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            torch.nn.BatchNorm2d(3)\n",
    "        )\n",
    "        self._msra_init()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = getattr(self, 'layer0')(x)\n",
    "        for i in range(1, self.n_layers):\n",
    "            out = getattr(self, f'layer{i}')(out)\n",
    "\n",
    "        out = self.residual(out)\n",
    "        out += x\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    @property\n",
    "    def conv_layer(self):\n",
    "        return torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm2d(64),\n",
    "            torch.nn.SELU()\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def n_params(self):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    def _msra_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.4 ms, sys: 623 Âµs, total: 31 ms\n",
      "Wall time: 29.8 ms\n"
     ]
    }
   ],
   "source": [
    "%time model_vd = DMCNN_VD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 33, 33]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 33, 33]             128\n",
      "              SELU-3           [-1, 64, 33, 33]               0\n",
      "            Conv2d-4           [-1, 64, 33, 33]          36,928\n",
      "       BatchNorm2d-5           [-1, 64, 33, 33]             128\n",
      "              SELU-6           [-1, 64, 33, 33]               0\n",
      "            Conv2d-7           [-1, 64, 33, 33]          36,928\n",
      "       BatchNorm2d-8           [-1, 64, 33, 33]             128\n",
      "              SELU-9           [-1, 64, 33, 33]               0\n",
      "           Conv2d-10           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-11           [-1, 64, 33, 33]             128\n",
      "             SELU-12           [-1, 64, 33, 33]               0\n",
      "           Conv2d-13           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-14           [-1, 64, 33, 33]             128\n",
      "             SELU-15           [-1, 64, 33, 33]               0\n",
      "           Conv2d-16           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-17           [-1, 64, 33, 33]             128\n",
      "             SELU-18           [-1, 64, 33, 33]               0\n",
      "           Conv2d-19           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-20           [-1, 64, 33, 33]             128\n",
      "             SELU-21           [-1, 64, 33, 33]               0\n",
      "           Conv2d-22           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-23           [-1, 64, 33, 33]             128\n",
      "             SELU-24           [-1, 64, 33, 33]               0\n",
      "           Conv2d-25           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-26           [-1, 64, 33, 33]             128\n",
      "             SELU-27           [-1, 64, 33, 33]               0\n",
      "           Conv2d-28           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-29           [-1, 64, 33, 33]             128\n",
      "             SELU-30           [-1, 64, 33, 33]               0\n",
      "           Conv2d-31           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-32           [-1, 64, 33, 33]             128\n",
      "             SELU-33           [-1, 64, 33, 33]               0\n",
      "           Conv2d-34           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-35           [-1, 64, 33, 33]             128\n",
      "             SELU-36           [-1, 64, 33, 33]               0\n",
      "           Conv2d-37           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-38           [-1, 64, 33, 33]             128\n",
      "             SELU-39           [-1, 64, 33, 33]               0\n",
      "           Conv2d-40           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-41           [-1, 64, 33, 33]             128\n",
      "             SELU-42           [-1, 64, 33, 33]               0\n",
      "           Conv2d-43           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-44           [-1, 64, 33, 33]             128\n",
      "             SELU-45           [-1, 64, 33, 33]               0\n",
      "           Conv2d-46           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-47           [-1, 64, 33, 33]             128\n",
      "             SELU-48           [-1, 64, 33, 33]               0\n",
      "           Conv2d-49           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-50           [-1, 64, 33, 33]             128\n",
      "             SELU-51           [-1, 64, 33, 33]               0\n",
      "           Conv2d-52           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-53           [-1, 64, 33, 33]             128\n",
      "             SELU-54           [-1, 64, 33, 33]               0\n",
      "           Conv2d-55           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-56           [-1, 64, 33, 33]             128\n",
      "             SELU-57           [-1, 64, 33, 33]               0\n",
      "           Conv2d-58           [-1, 64, 33, 33]          36,928\n",
      "      BatchNorm2d-59           [-1, 64, 33, 33]             128\n",
      "             SELU-60           [-1, 64, 33, 33]               0\n",
      "           Conv2d-61            [-1, 3, 33, 33]           1,728\n",
      "      BatchNorm2d-62            [-1, 3, 33, 33]               6\n",
      "             ReLU-63            [-1, 3, 33, 33]               0\n",
      "           Conv2d-64            [-1, 3, 33, 33]              81\n",
      "      BatchNorm2d-65            [-1, 3, 33, 33]               6\n",
      "================================================================\n",
      "Total params: 707,805\n",
      "Trainable params: 707,805\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 32.03\n",
      "Params size (MB): 2.70\n",
      "Estimated Total Size (MB): 34.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model_vd, input_size=(3, 33, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.068181818181817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "707e3 / 44e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMCNN-VD is roughly 16x larger model than the shallow DMCNN but it's still a tiny compared to some of the standard networks like VGG16 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The DMCNN-VD uses the same $L_2$ norm for training. However the original CFA is summed to the models residual output which we take into account when already defined the model.\n",
    "\n",
    "For the optimizer Adam is used with lr parameter $1^{-5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_vd = torch.optim.Adam(model_vd.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we'll choose a much larger epoch number and train the model in a similar manner to DMCNN. However we'll be dropping the clipping as the residual learning is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-35e10e998349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcfa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = []\n",
    "    for idx, (cfa, target) in enumerate(data_loader):\n",
    "        outputs = model_vd(cfa)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss_list.append(loss.item())\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer_vd.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_vd.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{n_epochs}], Step [{idx}/{total_step}], Loss: {loss.item()}')\n",
    "    epoch_stats = np.array(epoch_loss)\n",
    "    print(f'\\nFinished Epoch {epoch}, Loss --- mean: {epoch_stats.mean()}, std {epoch_stats.std()}\\n')\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,8))\n",
    "    ax1.imshow(np.array(outputs[-1].tolist()).reshape((21, 21, 3)))\n",
    "    ax2.imshow(np.array(cfa[-1].tolist()).reshape((33, 33, 3)))\n",
    "    ax3.imshow(np.array(target[-1].tolist()).reshape((21, 21, 3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmcnn",
   "language": "python",
   "name": "dmcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
