{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Deep Convolutional Networks for Demosaicing\n",
    "\n",
    "> Original paper by Nai-Sheng Syu, Yu-SHeng Chen, Yung-Yu Chuang [[1]](https://arxiv.org/pdf/1802.03769.pdf)\n",
    "\n",
    "> Homepage for the paper: [link](http://www.cmlab.csie.ntu.edu.tw/project/Deep-Demosaic/) (As of 21.4.2019 the authors have not released the original code)\n",
    "\n",
    "### About the notebook\n",
    "\n",
    "This notebook and repository aims to reproduce the state-of-the-art results provided in the [1]. \n",
    "\n",
    "**Author:** Eemeli Saari\n",
    "\n",
    "**Email:** saari.eemeli@gmail.com\n",
    "\n",
    "**Modified:** 21.4.2019\n",
    "\n",
    "> **NOTE:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set a standard seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f37226b7690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We load the data from the authors homepage and define a Pytorch dataset that reads patches from the images.\n",
    "\n",
    "The image patches are densely extracted $33 \\times 33$ patches that are padded with zeros to make up for the missing channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_data, ImagePatchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 ms, sys: 14.5 ms, total: 37.5 ms\n",
      "Wall time: 41.8 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8526"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time dataset = ImagePatchDataset(root=download_data(), sample_size=2)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMCNN Model\n",
    "\n",
    "We'll first define the DMCNN model described by the paper [1].\n",
    "\n",
    "Model contains\n",
    "\n",
    "- Feature extraction layer\n",
    "  - 128 Filters with $9\\times9$ kernels.\n",
    "- Non-linear mapping layer\n",
    "  - 64 Filters with $1\\times1$ kernels.\n",
    "- Reconstruction layer\n",
    "  - 3 Kernels constructing the resulted colored image\n",
    "  - $5\\times5$ kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMCNN(torch.nn.Module):\n",
    "    \"\"\"DMCNN\n",
    "    \n",
    "    Model adopted from [link](https://arxiv.org/pdf/1802.03769.pdf)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DMCNN, self).__init__()\n",
    "        \n",
    "        self.feature_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 128, kernel_size=9),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mapping_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(128, 64, kernel_size=1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.reconstruction_layer = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64, 3, kernel_size=5),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.feature_layer(x)\n",
    "        out = self.mapping_layer(out)\n",
    "        out = self.reconstruction_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 ms, sys: 2.7 ms, total: 4.36 ms\n",
      "Wall time: 3.67 ms\n"
     ]
    }
   ],
   "source": [
    "%time model = DMCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 25, 25]          31,232\n",
      "              ReLU-2          [-1, 128, 25, 25]               0\n",
      "            Conv2d-3           [-1, 64, 25, 25]           8,256\n",
      "              ReLU-4           [-1, 64, 25, 25]               0\n",
      "            Conv2d-5            [-1, 3, 21, 21]           4,803\n",
      "              ReLU-6            [-1, 3, 21, 21]               0\n",
      "================================================================\n",
      "Total params: 44,291\n",
      "Trainable params: 44,291\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.85\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 2.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 33, 33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see it's a fairly shallow convolution neural network that should be fairly fast to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "The Pytorch framework doesn't offer the similar sklearn type API that's supported in the Keras so we'll need to manually create the training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll define the the loss function as the standard $L_2$ norm.\n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{n} \\sum_{n}^{n=1}||F(Y_i ; \\theta) - X_i||^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original paper the DMCNN is trained using the stochastic gradient descent with learning rate 1 for the first two layers and 0.1 for the last layer.\n",
    "\n",
    "This comes easy to define in Pytorch as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    params = [\n",
    "        {\"params\": model.feature_layer.parameters(), \"lr\": 1},\n",
    "        {\"params\": model.mapping_layer.parameters(), \"lr\": 1},\n",
    "        {\"params\": model.reconstruction_layer.parameters(), \"lr\": 0.1}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define the training loop to include the clipping to keep the gradients within -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8ee9ba1ea814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcfa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "loss_list = []\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = []\n",
    "    for idx, (cfa, target) in enumerate(data_loader):\n",
    "        outputs = model(cfa)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss_list.append(loss.item())\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            #print(outputs)\n",
    "            print(f'Epoch [{epoch}/{n_epochs}], Step [{idx}/{total_step}], Loss: {loss.item()}')\n",
    "    epoch_stats = np.array(epoch_loss)\n",
    "    print(f'\\nFinished Epoch {epoch}, Loss --- mean: {epoch_stats.mean()}, std {epoch_stats.std()}\\n')\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12,8))\n",
    "    ax1.imshow(np.array(outputs[-1].tolist()).reshape((21, 21, 3)))\n",
    "    ax2.imshow(np.array(cfa[-1].tolist()).reshape((33, 33, 3)))\n",
    "    ax3.imshow(np.array(target[-1].tolist()).reshape((21, 21, 3)))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmcnn",
   "language": "python",
   "name": "dmcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
